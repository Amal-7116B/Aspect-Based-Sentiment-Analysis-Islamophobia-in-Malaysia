{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAu31KBCRxAI"
   },
   "source": [
    "#deberta-v3-base-abas-v1.1\n",
    "https://medium.com/nlplanet/quick-intro-to-aspect-based-sentiment-analysis-c8888a09eda7\n",
    "\n",
    "- the dataset is unlabeled, this study uses a pre-trained ABSA model yangheng/deberta-v3-base-absa-v1.1 to generate pseudo ground truth sentiment labels.\n",
    "- this model trained on SemEval datasets, and can detect aspect terms and classify the associated sentiment into pos/neu/neg\n",
    "- DeBERTa assign sentiment polarity and cofidence score (for fine tuning and evaluation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tuF5yenR0Nv"
   },
   "source": [
    "####Using a public pre-trained model\n",
    "First, we install the transformers library along with the SentencePiece tokenizer (which is needed by some models of the library, such as DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers[sentencepiece]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAfNh6blHBBk"
   },
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the tokenizer and ABSA model\n",
    "absa_tokenizer = AutoTokenizer.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\")\n",
    "absa_model = AutoModelForSequenceClassification.from_pretrained(\"yangheng/deberta-v3-base-absa-v1.1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV (update path if needed)\n",
    "df = pd.read_csv(\"ASPECT_preprocessed.csv\")\n",
    "\n",
    "# Keep only needed columns and remove duplicates\n",
    "df = df[['Id', 'cleaned_text', 'aspect']].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to eval mode\n",
    "absa_model.eval()\n",
    "\n",
    "# Store results here\n",
    "results = []\n",
    "\n",
    "# Loop through each row\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    sentence = row['cleaned_text']\n",
    "    aspect = row['aspect']\n",
    "\n",
    "    # ABSA-specific input format\n",
    "    encoded_input = absa_tokenizer(f\"[CLS] {sentence} [SEP] {aspect} [SEP]\", return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = absa_model(**encoded_input)\n",
    "        probs = F.softmax(output.logits, dim=1).detach().numpy()[0]\n",
    "\n",
    "    # Map probabilities to sentiment\n",
    "    sentiment_labels = ['negative', 'neutral', 'positive']\n",
    "    sentiment = sentiment_labels[probs.argmax()]\n",
    "    confidence = probs.max()\n",
    "\n",
    "    results.append({\n",
    "        \"Id\": row['Id'],\n",
    "        \"cleaned_text\": sentence,\n",
    "        \"aspect\": aspect,\n",
    "        \"sentiment\": sentiment,\n",
    "        \"confidence\": confidence\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame and save\n",
    "results_df.to_csv(\"DeBERTa.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvJiQ8I0Xrz3"
   },
   "source": [
    "####Merge file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "aspect_df = pd.read_csv(\"ASPECT_preprocessed.csv\")\n",
    "deberta_df = pd.read_csv(\"DeBERTa.csv\")\n",
    "\n",
    "# Merge on 'Id', 'translated_text', and 'aspect'\n",
    "merged_df = pd.merge(\n",
    "    aspect_df[['Id', 'created_at', 'cleaned_text', 'type', 'aspect']],\n",
    "    deberta_df[['Id', 'cleaned_text', 'aspect', 'sentiment', 'confidence']],\n",
    "    # Remove 'type' from the 'on' list as it's not in both dataframes\n",
    "    on=['Id', 'cleaned_text', 'aspect'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Rename 'sentiment' to 'deberta_sentiment'\n",
    "merged_df.rename(columns={'sentiment': 'deberta_sentiment'}, inplace=True)\n",
    "\n",
    "# Reorder columns\n",
    "merged_df = merged_df[['Id', 'created_at', 'cleaned_text', 'type', 'aspect', 'deberta_sentiment', 'confidence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the result to a new CSV file\n",
    "merged_df.to_csv(\"ABSA-DeBERTa_annotate.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
